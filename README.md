# Mini RAG + Streamlit (Dummy Project)

Proyek RAG mini dengan UI sederhana pakai **Streamlit**. Cocok buat demo / dipush ke GitHub.
Data dummy berisi FAQ produk fiktif *Komeng Workspace*.

## Fitur
- ğŸ” Retrieval via FAISS + `sentence-transformers` (`all-MiniLM-L6-v2`).
- ğŸ§  LLM offline default: `google/flan-t5-small` (via `transformers`).
- â˜ï¸ (Opsional) LLM OpenAI jika `OPENAI_API_KEY` tersedia.
- ğŸ–¥ï¸ UI sederhana ala chat + daftar sumber.
- ğŸ“¦ Script `ingest.py` untuk membangun index dari folder `data/`.

Cocok untuk sektor apa?

##RAG bersifat lintas-domain. Contoh sektor & use case:

Sektor	Use Case	Dampak
| Sektor           | Use Case                                           | Dampak                                                     |
|------------------|----------------------------------------------------|------------------------------------------------------------|
| Customer Support | FAQ, troubleshooting, kebijakan pengembalian       | Jawaban konsisten & cepat, beban tiket berkurang           |
| Sales/CS         | Q&A tentang produk, harga, perbandingan            | Respons cepat, bantu discovery & objection handling        |
| HR               | Kebijakan cuti, benefit, SOP onboarding            | Kurangi pertanyaan berulang, self-service karyawan         |
| IT/Helpdesk      | Runbook, how-to tools, incident tips               | MTTR turun, dokumentasi hidup                              |
| Legal/Compliance | Kebijakan, ketentuan, ringkasan klausul            | Akses cepat ke referensi resmi                             |
| Edu/Training     | Materi pelatihan, modul belajar internal           | Pembelajaran kontekstual dari materi internal              |

Catatan: proyek ini adalah dummy; ganti dataset data/ dengan dokumen organisasi Anda agar relevan.

## Cara jalanin (lokal)
```bash
python -m venv .venv && source .venv/bin/activate  # (Windows: .venv\Scripts\activate)
pip install -r requirements.txt

# Bangun index dari data/ (boleh di-skip, app akan otomatis build saat pertama kali)
python ingest.py

# Jalankan UI
streamlit run app.py
```

## Cara memakai fitur (end-user)

1. Masukkan pertanyaan di input chat.
2. Atur Top-K (berapa banyak potongan konteks yang dipakai; default 4). Nilai lebih tinggi = lebih luas, tapi bisa lebih "bising".
3. Atur Temperature (0.0â€“1.5). Lebih rendah = jawaban lebih deterministik.
4. Pilih Backend: Offline (cepat, tanpa API) atau OpenAI (jika tersedia untuk kualitas lebih baik).
5. Lihat panel Sumber untuk verifikasi fakta.
6. Klik â€œBangun ulang indexâ€ di sidebar jika mengganti isi data/.

## Struktur repo
```
mini-rag-ui/
â”œâ”€ app.py                # Streamlit app
â”œâ”€ ingest.py             # Build index dari folder data/
â”œâ”€ rag/
â”‚  â”œâ”€ __init__.py
â”‚  â”œâ”€ retriever.py       # FAISS + embeddings + search
â”‚  â”œâ”€ llm.py             # Backend LLM (transformers atau OpenAI)
â”‚  â””â”€ utils.py           # helper kecil
â”œâ”€ data/                 # dokumen dummy (markdown)
â”œâ”€ store/                # index FAISS + metadata (dibuat otomatis)
â”œâ”€ tests/
â”‚  â””â”€ test_retriever.py  # contoh test sederhana
â”œâ”€ .streamlit/
â”‚  â””â”€ config.toml        # tema
â”œâ”€ requirements.txt
â”œâ”€ .gitignore
â””â”€ LICENSE
```

## Menambah dokumen
- Tambahkan file `.md` atau `.txt` ke folder `data/`.
- Jalankan `python ingest.py` (atau cukup restart `streamlit run app.py` â€” akan auto-build jika index belum ada).

## Variabel lingkungan (opsional)
- `OPENAI_API_KEY` â€” kalau ada, app akan menawarkan backend OpenAI (`gpt-4o-mini` secara default).
- `MODEL_NAME` â€” ubah model offline (mis. `google/flan-t5-base`) bila mau.

## Catatan performa
- Model offline kecil (flan-t5-small) jalan di CPU, cukup untuk demo.
- Untuk dataset besar atau latensi rendah, sebaiknya pakai layanan LLM hosted.

## FAQ singkat

Q: Perlu GPU?
A: Tidak. Proyek demo jalan di CPU. GPU mempercepat, tapi tak wajib.

Q: Kenapa jawaban kadang generik?
A: Perbesar kualitas data; kurangi Temperature; tambah Top-K secukupnya; pertimbangkan model yang lebih kuat.

Q: Bisakah membatasi agar hanya menjawab dari konteks?
A: Yaâ€”logika prompt sudah mengarahkan. Bisa diperketat (mis. tolak jika konteks kosong).

## Lisensi
MIT
